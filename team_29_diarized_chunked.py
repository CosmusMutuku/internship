# -*- coding: utf-8 -*-
"""Team_29_diarized_chunked.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10A5TsfTqelB4Fr8mWsEJDQEUq66XG62-

## Installations
"""

!pip install pytube -q
!pip install huggingsound -q
!pip install -q transformers




import librosa
import torch
import warnings
import soundfile as sf
warnings.filterwarnings('ignore')


from pytube import YouTube


device = "cuda" if torch.cuda.is_available() else "cpu"

"""### Getting the TedTalk Video From Youtube"""

#Install pytube package and YouTube to aid getting YouTube videos

#Give the youtube video link to be converted and transcripted
video_link="https://www.youtube.com/watch?v=3WrZMzqpFTc"

#Get the video
yt=YouTube(video_link)
yt.streams \
  .filter(only_audio = True, file_extension = 'mp4') \
  .first() \
  .download(filename = 'ytaudio.mp4')

"""### Converting Video to Audio"""

#We use FFmpeg to convert the video to audio
! ffmpeg -i ytaudio.mp4 -acodec pcm_s16le -ar 16000 ytaudio.wav

"""### Converting Audio to Text

### Pre-Processing

#### 1. Removing Silence
"""

# Using Librosa.effects.trim


import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns

from glob import glob

import librosa
import librosa.display
import IPython.display as ipd

from itertools import cycle

sns.set_theme(style="white", palette=None)
color_pal = plt.rcParams["axes.prop_cycle"].by_key()["color"]
color_cycle = cycle(plt.rcParams["axes.prop_cycle"].by_key()["color"])

"""##### Getting features of the audio file"""

input_file = 'ytaudio.wav'
print(librosa.get_samplerate(input_file))

# audio_file = glob('ytaudio.wav')

y, sr = librosa.load('ytaudio.wav')
print(f'y: {y[:100]}')
print(f'shape y: {y.shape}')
print(f'sr: {sr}')

pd.Series(y).plot(figsize=(10, 5),
                  lw=1,
                  title='Raw Audio',
                 color=color_pal[0])
plt.show()

# Trimming leading/lagging silence
y_trimmed, _ = librosa.effects.trim(y, top_db=1000)
pd.Series(y_trimmed).plot(figsize=(10, 5),
                  lw=1,
                  title='Raw Audio Trimmed',
                 color=color_pal[2])
plt.show()

pd.Series(y_trimmed[30000:30500]).plot(figsize=(10, 5),
                  lw=1,
                  title='Raw Audio Zoomed In',
                 color=color_pal[3])
plt.show()

#Spectrum of the audio
import numpy as np
#Fourier Transformation
fft= np.fft.fft(y)

#magnitute
magnitude=np.abs(fft)
#frequency
frequency=np.linspace(0, sr, len(magnitude))

left_frequency=frequency[:int(len(frequency)/2)]
left_magnitude=magnitude[:int(len(frequency)/2)]

plt.plot(left_frequency, left_magnitude)
plt.xlabel("frequency")
plt.ylabel("Amplitude")
plt.show()

#Short TFT of the audio
n_fft=2048
hop_length=512

stft=librosa.core.stft(y, hop_length=hop_length, n_fft=n_fft)

spectrogram=np.abs(stft)
 #plot
librosa.display.specshow(spectrogram, sr=sr, hop_length=hop_length)

plt.xlabel("time")
plt.ylabel("frequency")
plt.colorbar()
plt.show()

# Play audio file
ipd.Audio('ytaudio.wav')

# Play audio file
ipd.Audio(y_trimmed)

"""#### 2. Removing Noise"""



"""#### 3. Audio Chunking"""

# Stream over 30 seconds chunks rather than load the full file
stream = librosa.stream(
    input_file,
    block_length=30,
    frame_length=16000,
    hop_length=16000
)

for i,speech in enumerate(stream):
  sf.write(f'{i}.wav', speech, 16000)
audio_path =[]
for a in range(i+1):
  audio_path.append(f'{a}.wav')

audio_path

ipd.Audio('ytaudio.wav')

"""#### 4. Speaker Diarization"""

!pip install Resemblyzer
from resemblyzer import preprocess_wav, VoiceEncoder
from pathlib import Path

#give the file path to your audio file
audio_file_path = "ytaudio.wav"
wav_fpath = Path(audio_file_path)

wav = preprocess_wav(wav_fpath)
encoder = VoiceEncoder("cpu")
_, cont_embeds, wav_splits = encoder.embed_utterance(wav, return_partials=True, rate=16)
print(cont_embeds.shape)

!pip3 install spectralcluster
from spectralcluster import SpectralClusterer
from spectralcluster import RefinementOptions

refinement_options = RefinementOptions(
    gaussian_blur_sigma=1,
    p_percentile=0.90)
clusterer = SpectralClusterer(
    min_clusters=2,
    max_clusters=100)

labels = clusterer.predict(cont_embeds)

labels

def create_labelling(labels,wav_splits):
    from resemblyzer.audio import sampling_rate
    times = [((s.start + s.stop) / 2) / sampling_rate for s in wav_splits]
    labelling = []
    start_time = 0

    for i,time in enumerate(times):
        if i>0 and labels[i]!=labels[i-1]:
            temp = [str(labels[i-1]),start_time,time]
            labelling.append(list(temp))
            start_time = time
        if i==len(times)-1:
            temp = [str(labels[i]),start_time,time]
            labelling.append(list(temp))

    return labelling

labelling = create_labelling(labels,wav_splits)

labelling

labelling[0][2]

import pandas as pd

df_labels = pd.DataFrame(labelling, columns = ['Speaker', 'start_time', 'end_time'])

df_labels

print(df_labels.index)

labelling

audio_chunks2 = []
l=0
for k in range(len(labelling)):
  s=labelling[k][1]
  e=labelling[k][2]
  filename = str(l)+"ytaudio.wav"
  !ffmpeg -i ytaudio.wav -ss {s} -t {e} -acodec copy {filename}
  l=l+1
  audio_chunks2.append(filename)

audio_chunks2

filename

#audio_file= "your_wav_file.wav"
#audio = AudioSegment.from_wav(audio_file)
#list_of_timestamps = [ 10, 20, 30, 40, 50 ,60, 70, 80, 90 ] #and so on in *seconds*

#start = ""
#for  idx,t in enumerate(list_of_timestamps):
    #break loop if at last element of list
    #if idx == len(list_of_timestamps):
        #break

    #end = t * 1000 #pydub works in millisec
    #print "split at [ {}:{}] ms".format(start, end)
    #audio_chunk=audio[start:end]
    #audio_chunk.export( "audio_chunk_{}.wav".format(end), format="wav")

    #start = end * 1000 #pydub works in millisec













ipd.Audio('16.wav')

#Extract MFCCs
3mfccs=librosa.feature.mfcc(y, n_mfcc=13, sr=sr)
mfccs.shape

#visualize mfcc
plt.figure(figsize=(15,26))
librosa.display.specshow(mfccs,
                         x_axis="time", sr=sr)
plt.colorbar(format="%+2f")
plt.show()

#Calculate delta and delta mfss
delta_mfccs=librosa.feature.delta(mfccs)
delta2_mfccs=librosa.feature.delta(mfccs, order=2)

delta_mfccs.shape

delta2_mfccs.shape

#visualize delta MFCCS
librosa.display.specshow(delta_mfccs,
                         x_axis="time", sr=sr)
plt.colorbar(format="%+2f")
plt.show()

#visualize delta2_mfccs
librosa.display.specshow(delta2_mfccs,
                         x_axis="time", sr=sr)
plt.colorbar(format="%+2f")
plt.show()

#concation (comprehensive mfccs)
comprehensive_mfccs=np.concatenate((mfccs, delta_mfccs, delta2_mfccs))

#visualize comprehensive_mfccs
librosa.display.specshow(comprehensive_mfccs)
plt.colorbar(format="%+2f")
plt.show()

chroma_stft = librosa.feature.chroma_stft(y=y_trimmed, sr=sr)
chroma_stft.shape

librosa.display.specshow(chroma_stft)
plt.colorbar(format="%+2f")
plt.show()

rmse = librosa.feature.rms(y=y)
rmse.shape

librosa.display.specshow(rmse)
plt.colorbar(format="%+2f")
plt.show()

spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)
spec_cent.shape

librosa.display.specshow(spec_cent)
plt.colorbar(format="%+2f")
plt.show()

spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
librosa.display.specshow(spec_bw)
plt.colorbar(format="%+2f")
plt.show()

rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
librosa.display.specshow(rolloff)
plt.colorbar(format="%+2f")
plt.show()

zcr = librosa.feature.zero_crossing_rate(y)
librosa.display.specshow(zcr)
plt.colorbar(format="%+2f")
plt.show()



"""#### Model 1: Speech Recognition Model from Hugging Sound"""

#We use speech recognition model from hugging sound to do the transcription
from huggingsound import SpeechRecognitionModel

#specify the recognition model
model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-large-xlsr-53-english", device = device)

transcriptions = model.transcribe(audio_chunks2)

for x, text in enumerate(transcription):
    print(text)
    print()

full_transcript = ' '
for item in transcriptions:
  full_transcript += ''.join(item['transcription'])

len(full_transcript)

#Check the full transcript
full_transcript

!sudo apt-get install swig
!sudo pip install jamspell

!wget https://github.com/bakwc/JamSpell-models/raw/master/en.tar.gz
!tar -xvf en.tar.gz
import jamspell
jsp = jamspell.TSpellCorrector()
assert jsp.LoadLangModel('en.bin')

full_transcript

full_transcript
jsp.FixFragment(full_transcript)

"""###Model 1 Evaluation"""

with open("/content/Ground_Truth.txt", 'r') as f:
  ground_truth = f.read()

ground_trth_WC = ground_truth.join(" ")
len(ground_trth_WC)

ground_truth

!pip install jiwer
from jiwer import wer
import jiwer

transformation = jiwer.Compose([
    jiwer.ToLowerCase(),
    jiwer.RemoveWhiteSpace(replace_by_space=True),
    jiwer.RemoveMultipleSpaces(),
    jiwer.ReduceToListOfListOfWords(word_delimiter=" ")
])

jiwer.wer(
    ground_truth,
    full_transcript,
    truth_transform=transformation,
    hypothesis_transform=transformation
)

"""#### Model 2: wav2vec2-xls-r-1b-english"""

from huggingsound import SpeechRecognitionModel

model = SpeechRecognitionModel("jonatasgrosman/wav2vec2-xls-r-1b-english")


transcriptions = model.transcribe(audio_path)

full_transcript = ' '
for item in transcriptions:
  full_transcript += ''.join(item['transcription'])

full_transcript

!wget https://github.com/bakwc/JamSpell-models/raw/master/en.tar.gz
!tar -xvf en.tar.gz
import jamspell
jsp = jamspell.TSpellCorrector()
assert jsp.LoadLangModel('en.bin')

full_transcript
jsp.FixFragment(full_transcript)

transformation = jiwer.Compose([
    jiwer.ToLowerCase(),
    jiwer.RemoveWhiteSpace(replace_by_space=True),
    jiwer.RemoveMultipleSpaces(),
    jiwer.ReduceToListOfListOfWords(word_delimiter=" ")
])

jiwer.wer(
    ground_truth,
    full_transcript,
    truth_transform=transformation,
    hypothesis_transform=transformation
)

"""# ####Model 3: Speechbrain model"""

!pip install speechbrain
from speechbrain.pretrained import EncoderDecoderASR

asr_model = EncoderDecoderASR.from_hparams(source="speechbrain/asr-crdnn-rnnlm-librispeech", savedir="pretrained_models/asr-crdnn-rnnlm-librispeech")

transcriptions=asr_model.transcribe_file('12.wav')
transcriptions

full_transcript = ' '
for item in transcriptions:
  full_transcript += ''.join(item[1])
full_transcript

"""##### Vosk Model"""

!pip install vosk

FRAME_RATE=16000

from vosk import Model, KaldiRecognizer
model=Model(model_name='vosk-model-en-us-0.22-lgraph')
rec=KaldiRecognizer(model, FRAME_RATE)

#show the complete transcript and individual words
rec.SetWords(True)

rec.AcceptWaveform(audio_path)

"""#### Model 3: DeepSpeech Model"""

!pip install deepspeech==0.8.2
!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.pbmm
!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-models.scorer

#dependencies
!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg

#importations
from deepspeech import Model
import numpy as np
import os
import wave
from IPython.display import Audio

# Create virtual environment named ds082
!python3 -m venv ./some/pyenv/dir/path/ds082
# Switch to virtual environment
!source ./some/pyenv/dir/path/ds082/bin/activate

# Download and unzip en-US models, this will take a while
!mkdir -p ./some/workspace/path/ds082
!cd ./some/workspace/path/ds082
!mkdir deepspeech-0.8.2-models
!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.1/deepspeech-0.8.1-models.pbmm
!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.1/deepspeech-0.8.1-models.scorer
!mv deepspeech-0.8.1-models.pbmm deepspeech-0.8.1-models.scorer deepspeech-0.8.2-models/
!ls -l ./deepspeech-0.8.2-models/
# Download and unzip some audio samples to test setup
! curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/audio-0.8.2.tar.gz
! tar -xvzf audio-0.8.2.tar.gz

! ls -l ./audio/
# Test deepspeech
! deepspeech --model deepspeech-0.8.2-models/deepspeech-0.8.1-models.pbmm --scorer deepspeech-0.8.2-models/deepspeech-0.8.1-models.scorer --audio ./audio/2830-3980-0043.wav
! deepspeech --model deepspeech-0.8.2-models/deepspeech-0.8.1-models.pbmm --scorer deepspeech-0.8.2-models/deepspeech-0.8.1-models.scorer --audio ./audio/4507-16021-0012.wav
! deepspeech --model deepspeech-0.8.2-models/deepspeech-0.8.1-models.pbmm --scorer deepspeech-0.8.2-models/deepspeech-0.8.1-models.scorer --audio ./audio/8455-210777-0068.wav

#Method 1 of transcription

#file
model_file_path = 'deepspeech-0.8.2-models.pbmm'
lm_file_path = 'deepspeech-0.8.2-models.scorer'
#initialize
beam_width = 100
lm_alpha = 0.93
lm_beta = 1.18

model = Model(model_file_path)
model.enableExternalScorer(lm_file_path)

model.setScorerAlphaBeta(lm_alpha, lm_beta)
model.setBeamWidth(beam_width)

stream = model.stt('')

#read the audio
def read_audio_file(audio_path):
 with wave.open(ytaudio.wav, 'rb') as w:
        rate = w.getframerate()
        frames = w.getnframes()
        buffer = w.readframes(frames)

 return buffer, rate

#Transcription
def real_time_transcription():
    buffer, rate =read_audio_file(ytaudio.wav)
    offset=0
    batch_size=8196
    text=''
    while offset < len(buffer):
      end_offset=offset+batch_size
      chunk=buffer[offset:end_offset]
      data16 = np.frombuffer(chunk, dtype=np.int16)

      stream.feedAudioContent(data16)
      text=stream.intermediateDecode()
      print(text)
      offset=end_offset
      return True

real_time_transcription('ytaudio.wav')

#Method 2 of transcription

model_file_path = 'deepspeech-0.8.2-models/deepspeech-0.8.1-models.pbmm'
#model = deepspeech.Model(model_file_path)
model = Model(model_file_path)

scorer_file_path = 'deepspeech-0.8.2-models/deepspeech-0.8.1-models.scorer'
model.enableExternalScorer(scorer_file_path)
lm_alpha = 0.75
lm_beta = 1.85
model.setScorerAlphaBeta(lm_alpha, lm_beta)
beam_width = 500
model.setBeamWidth(beam_width)

transcript=model.stt('ytaudio.wav')

print(model.sampleRate())

type(buffer)

full_transcripts=''
for item in audio_path:
  transcript=model.stt(item)
  for i in transcript:
    full_transcripts += ''.join(item['transcript'])

full_transcripts

"""### Model Perfomance"""

#JiWER: Similarity measures for automatic speech recognition evaluation
from datasets import load_dataset

import torch
from jiwer import wer


librispeech_eval = load_dataset("librispeech_asr", "clean", split="test")

model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self").to("cuda")
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")

def map_to_pred(batch):
    inputs = processor(batch["audio"]["array"], return_tensors="pt", padding="longest")
    input_values = inputs.input_values.to("cuda")
    attention_mask = inputs.attention_mask.to("cuda")

    with torch.no_grad():
        logits = model(input_values, attention_mask=attention_mask).logits

    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)
    batch["transcription"] = transcription
    return batch

result = librispeech_eval.map(map_to_pred, remove_columns=["audio"])

print("WER:", wer(result["text"], result["transcription"]))



"""####Text Post-Processing"""

#using RecasePunc